{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9fedfeb",
   "metadata": {},
   "source": [
    "# Implementação do modelo NCF: GMF, MLP e NeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b2d2f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545adb74",
   "metadata": {},
   "source": [
    "## Importação da base de dados MovieLens 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a35ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando o dataset MovieLens 100k...\n",
      "Descompactando o arquivo...\n",
      "Dataset MovieLens 100k baixado e descompactado na pasta 'ml-100k'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# URL do dataset MovieLens 100k\n",
    "url = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "dataset_dir = \"ml-100k\"\n",
    "\n",
    "# Nome do arquivo que será baixado\n",
    "filename = \"ml-100k.zip\"\n",
    "\n",
    "# Verifica se o diretório já existe\n",
    "if not os.path.exists(dataset_dir):\n",
    "    # Faz o download do arquivo zip\n",
    "    print(\"Baixando o dataset MovieLens 100k...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    \n",
    "    # Descompacta o arquivo zip\n",
    "    print(\"Descompactando o arquivo...\")\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "\n",
    "    # Remove o arquivo zip após descompactar\n",
    "    os.remove(filename)\n",
    "\n",
    "    print(f\"Dataset MovieLens 100k baixado e descompactado na pasta '{dataset_dir}'.\")\n",
    "else:\n",
    "    print(f\"O dataset já existe na pasta '{dataset_dir}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f9d9b",
   "metadata": {},
   "source": [
    "## Implementação das classes GMF e MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80225eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim):\n",
    "        nn.Module.__init__(self)\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "                 \n",
    "        # COMENTAR SE FOR USAR NeuMF       \n",
    "        # Definir a camada de saída\n",
    "        self.output = nn.Linear(embedding_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # COMENTAR SE FOR USAR NeuMF\n",
    "        \n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        user_emb = self.user_embedding(user)\n",
    "        item_emb = self.item_embedding(item)\n",
    "        \n",
    "        output = user_emb * item_emb  # Produto de Hadamard\n",
    "        \n",
    "        # COMENTAR SE FOR USAR NeuMF\n",
    "        # Passar pela camada de saída e aplicar sigmoid\n",
    "        output = self.sigmoid(self.output(output))\n",
    "        # COMENTAR SE FOR USAR NeuMF\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim, hidden_layers=[64, 32, 16, 8]):\n",
    "        nn.Module.__init__(self)\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "        \n",
    "        layers = []\n",
    "        input_size = embedding_dim * 2\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = hidden_size\n",
    "        \n",
    "        self.mlp_layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, user, item):\n",
    "        user_emb = self.user_embedding(user)\n",
    "        item_emb = self.item_embedding(item)\n",
    "        mlp_input = torch.cat([user_emb, item_emb], dim=-1)\n",
    "        mlp_output = self.mlp_layers(mlp_input)\n",
    "        return mlp_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bfb0b0",
   "metadata": {},
   "source": [
    "## Implementação da classe NeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76b5f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(GMF, MLP):\n",
    "    def __init__(self, num_users, num_items, embedding_dim, hidden_layers=[64, 32, 16, 8], pretrained_gmf=None, pretrained_mlp=None):\n",
    "        nn.Module.__init__(self)  # Inicializa o nn.Module, necessário em herança múltipla\n",
    "        GMF.__init__(self, num_users, num_items, embedding_dim)\n",
    "        MLP.__init__(self, num_users, num_items, embedding_dim, hidden_layers)\n",
    "        \n",
    "        # Camada final para combinar as saídas do GMF e MLP\n",
    "        combined_input_size = embedding_dim + hidden_layers[-1]\n",
    "        self.output_layer = nn.Linear(combined_input_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Carregar pesos pré-treinados se fornecidos\n",
    "        if pretrained_gmf:\n",
    "            self.gmf_user_embedding.weight.data.copy_(pretrained_gmf.user_embedding.weight)\n",
    "            self.gmf_item_embedding.weight.data.copy_(pretrained_gmf.item_embedding.weight)\n",
    "        if pretrained_mlp:\n",
    "            self.mlp_user_embedding.weight.data.copy_(pretrained_mlp.user_embedding.weight)\n",
    "            self.mlp_item_embedding.weight.data.copy_(pretrained_mlp.item_embedding.weight)\n",
    "            self.mlp_layers.load_state_dict(pretrained_mlp.mlp_layers.state_dict())\n",
    "    \n",
    "    def forward(self, user, item):\n",
    "        # GMF forward pass\n",
    "        gmf_output = GMF.forward(self, user, item)\n",
    "        \n",
    "        # MLP forward pass\n",
    "        mlp_output = MLP.forward(self, user, item)\n",
    "        \n",
    "        # Concatenar as saídas de GMF e MLP\n",
    "        combined_output = torch.cat([gmf_output, mlp_output], dim=-1)\n",
    "        \n",
    "        # Passar pela camada de saída e aplicar sigmoid\n",
    "        output = self.sigmoid(self.output_layer(combined_output))\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238fcb52",
   "metadata": {},
   "source": [
    "## Carregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db729740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certifique-se de ajustar o caminho para o local onde você salvou o arquivo u.data\n",
    "df = pd.read_csv('ml-100k/u.data', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "# Remover a coluna de timestamp, pois não é necessária\n",
    "df = df.drop('timestamp', axis=1)\n",
    "\n",
    "# Converter IDs para índices consecutivos (necessário para o PyTorch)\n",
    "df['user_id'] = df['user_id'].astype('category').cat.codes.values\n",
    "df['item_id'] = df['item_id'].astype('category').cat.codes.values\n",
    "\n",
    "# Converter as avaliações para binárias (1 se rating >= 4, caso contrário 0)\n",
    "df['rating'] = (df['rating'] >= 4).astype(int)\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Converter para tensores PyTorch e garantir que sejam do tipo LongTensor\n",
    "train_users = torch.tensor(train_data['user_id'].values, dtype=torch.long)\n",
    "train_items = torch.tensor(train_data['item_id'].values, dtype=torch.long)\n",
    "train_ratings = torch.tensor(train_data['rating'].values).float()\n",
    "\n",
    "test_users = torch.tensor(test_data['user_id'].values, dtype=torch.long)\n",
    "test_items = torch.tensor(test_data['item_id'].values, dtype=torch.long)\n",
    "test_ratings = torch.tensor(test_data['rating'].values).float()\n",
    "\n",
    "# Criar DataLoader para facilitar o treinamento em mini-batches\n",
    "train_dataset = data.TensorDataset(train_users, train_items, train_ratings)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d7c5b",
   "metadata": {},
   "source": [
    "## Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b75ed290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.6917\n",
      "Epoch 2/20, Loss: 0.6415\n",
      "Epoch 3/20, Loss: 0.5670\n",
      "Epoch 4/20, Loss: 0.5319\n",
      "Epoch 5/20, Loss: 0.5137\n",
      "Epoch 6/20, Loss: 0.4999\n",
      "Epoch 7/20, Loss: 0.4876\n",
      "Epoch 8/20, Loss: 0.4758\n",
      "Epoch 9/20, Loss: 0.4636\n",
      "Epoch 10/20, Loss: 0.4509\n",
      "Epoch 11/20, Loss: 0.4371\n",
      "Epoch 12/20, Loss: 0.4222\n",
      "Epoch 13/20, Loss: 0.4062\n",
      "Epoch 14/20, Loss: 0.3894\n",
      "Epoch 15/20, Loss: 0.3723\n",
      "Epoch 16/20, Loss: 0.3551\n",
      "Epoch 17/20, Loss: 0.3387\n",
      "Epoch 18/20, Loss: 0.3227\n",
      "Epoch 19/20, Loss: 0.3082\n",
      "Epoch 20/20, Loss: 0.2945\n"
     ]
    }
   ],
   "source": [
    "# Definir os hiperparâmetros\n",
    "num_users = df['user_id'].nunique()\n",
    "num_items = df['item_id'].nunique()\n",
    "embedding_dim = 20  # Dimensão das embeddings\n",
    "hidden_layers = [64, 32, 16, 8]  # Estrutura das camadas ocultas do MLP\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# Instanciar o modelo NeuMF\n",
    "#model = NeuMF(num_users, num_items, embedding_dim, hidden_layers)\n",
    "\n",
    "model = GMF(num_users, num_items, embedding_dim)\n",
    "\n",
    "# Definir o otimizador e a função de perda\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# Treinamento do modelo\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for user, item, rating in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Passar os dados pelo modelo\n",
    "        prediction = model(user, item)\n",
    "        loss = criterion(prediction.squeeze(), rating)\n",
    "        \n",
    "        # Backpropagation e atualização dos pesos\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a43b4",
   "metadata": {},
   "source": [
    "## Avaliação no conjunto de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29abae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6902\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_prediction = model(test_users, test_items)\n",
    "    test_loss = criterion(test_prediction.squeeze(), test_ratings)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc81be",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
